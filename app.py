# app.py  â€” LGWORK ä¸€é”®è¦†ç›–ç‰ˆ
# -*- coding: utf-8 -*-

import csv
import io
import json
import os
import re
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Any

import requests
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

APP_NAME = "LGWORK"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PUBLIC_DIR = os.path.join(BASE_DIR, "public")

DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)
JOBS_CSV = os.path.join(DATA_DIR, "jobs.csv")

# ç¯å¢ƒå˜é‡ï¼ˆDeepSeek å¯é€‰ï¼‰
LLM_API_KEY = os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_API_KEY")
LLM_API_BASE = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com")
LLM_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
ADMIN_TOKEN = os.getenv("ADMIN_TOKEN")  # å¦‚æœä¸è®¾ç½®ï¼Œåˆ™ä¸åšé‰´æƒ

# CSV åˆ—å¤´
CSV_HEADERS = ["title", "company", "location", "jd_url", "source", "posted_at", "tags"]

# çº¿ç¨‹é”ï¼Œé¿å…å¹¶å‘å†™æ–‡ä»¶é—®é¢˜
_file_lock = threading.Lock()

app = Flask(__name__, static_folder="public", static_url_path="/public")
CORS(app)


# ---------- å·¥å…·å‡½æ•° ----------
def _log(msg: str):
    print(f"[{APP_NAME}] {msg}", flush=True)


def ensure_jobs_csv():
    """ç¡®ä¿ CSV æ–‡ä»¶å­˜åœ¨ä¸”æœ‰è¡¨å¤´"""
    if not os.path.exists(JOBS_CSV):
        with _file_lock:
            with open(JOBS_CSV, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(CSV_HEADERS)


def _safe_json_loads(s: str, default: Any) -> Any:
    if not s:
        return default
    try:
        return json.loads(s)
    except Exception:
        return default


def _normalize_url(u: str) -> str:
    # ç®€å•å½’ä¸€åŒ–ï¼Œå»æ‰å°¾éƒ¨ / å’Œç©ºç™½
    if not u:
        return u
    return u.strip().rstrip("/")


def _read_jobs() -> List[Dict[str, str]]:
    """å®¹é”™è¯» CSVï¼›tags è„ JSON ç›´æ¥è¿”å› []"""
    ensure_jobs_csv()
    rows = []
    with _file_lock:
        with open(JOBS_CSV, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            # å¦‚æœè¡¨å¤´ä¸å¯¹ï¼Œé‡æ–°å†™è¡¨å¤´
            if reader.fieldnames is None or set(reader.fieldnames) != set(CSV_HEADERS):
                # å¤‡ä»½æ—§æ–‡ä»¶
                f.seek(0)
                raw = f.read()
                _log("jobs.csv header invalid â€” rewriting header and preserving rows when possible.")
                with open(JOBS_CSV, "w", newline="", encoding="utf-8") as fw:
                    w = csv.writer(fw)
                    w.writerow(CSV_HEADERS)
                # å°½é‡ä¸å´©ï¼Œä½†æ— æ³•ä¿ç•™æ—§è¡Œç»“æ„ï¼Œè¿”å›ç©º
                return []
            for r in reader:
                # å…œåº•å­—æ®µ
                item = {
                    "title": (r.get("title") or "").strip(),
                    "company": (r.get("company") or "").strip(),
                    "location": (r.get("location") or "").strip(),
                    "jd_url": _normalize_url(r.get("jd_url") or ""),
                    "source": (r.get("source") or "").strip(),
                    "posted_at": (r.get("posted_at") or "").strip(),
                    "tags": json.dumps(_safe_json_loads(r.get("tags") or "", []), ensure_ascii=False),
                }
                rows.append(item)
    return rows


def _write_jobs(jobs: List[Dict[str, str]]):
    ensure_jobs_csv()
    with _file_lock:
        with open(JOBS_CSV, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=CSV_HEADERS)
            writer.writeheader()
            for j in jobs:
                row = {k: j.get(k, "") for k in CSV_HEADERS}
                # ç¡®ä¿ tags ä¸ºåˆæ³• JSON å­—ç¬¦ä¸²
                row["tags"] = json.dumps(_safe_json_loads(row.get("tags", ""), []), ensure_ascii=False)
                writer.writerow(row)


def _append_jobs(new_jobs: List[Dict[str, str]]) -> Tuple[int, int]:
    """è¿½åŠ èŒä½ï¼ŒæŒ‰ jd_url å»é‡ï¼›è¿”å› (æ–°å¢æ¡æ•°, å»é‡æ¡æ•°)"""
    existing = _read_jobs()
    seen = {e["jd_url"] for e in existing if e.get("jd_url")}
    added, skipped = 0, 0
    clean = []

    for j in new_jobs:
        # è§„èŒƒåŒ–
        j["jd_url"] = _normalize_url(j.get("jd_url", ""))
        j["posted_at"] = j.get("posted_at") or datetime.utcnow().isoformat()
        j["tags"] = json.dumps(_safe_json_loads(j.get("tags", ""), []), ensure_ascii=False)

        if not j["jd_url"] or j["jd_url"] in seen:
            skipped += 1
            continue
        seen.add(j["jd_url"])
        clean.append(j)
        added += 1

    if clean:
        existing.extend(clean)
        _write_jobs(existing)

    return added, skipped


def _require_admin(req: request):
    """å¯é€‰çš„ admin token æ ¡éªŒ"""
    if not ADMIN_TOKEN:
        return True
    token = req.headers.get("X-ADMIN-TOKEN") or req.args.get("admin_token") or (req.json or {}).get("admin_token")
    return token == ADMIN_TOKEN


# ---------- Greenhouse / Lever / Workday æŠ“å– ----------
def fetch_greenhouse(company: str) -> List[Dict[str, str]]:
    # API æ–‡æ¡£ï¼šhttps://developers.greenhouse.io/job-board.html#list-jobs
    url = f"https://boards-api.greenhouse.io/v1/boards/{company}/jobs?content=true"
    _log(f"Fetching Greenhouse: {url}")
    r = requests.get(url, timeout=20)
    r.raise_for_status()
    data = r.json() or {}
    out = []
    for job in data.get("jobs", []):
        out.append({
            "title": job.get("title", "").strip(),
            "company": company,
            "location": (job.get("location") or {}).get("name", ""),
            "jd_url": job.get("absolute_url", ""),
            "source": "greenhouse",
            "posted_at": job.get("updated_at") or job.get("created_at") or "",
            "tags": json.dumps([], ensure_ascii=False),
        })
    return out


def fetch_lever(company: str) -> List[Dict[str, str]]:
    # å…¬å¼€ JSONï¼š https://api.lever.co/v0/postings/<company>?mode=json
    url = f"https://api.lever.co/v0/postings/{company}?mode=json"
    _log(f"Fetching Lever: {url}")
    r = requests.get(url, timeout=20)
    r.raise_for_status()
    data = r.json() or []
    out = []
    for job in data:
        loc = ""
        if job.get("categories"):
            loc = job["categories"].get("location", "") or ""
        out.append({
            "title": job.get("text", "").strip(),
            "company": company,
            "location": loc,
            "jd_url": job.get("hostedUrl", ""),
            "source": "lever",
            "posted_at": job.get("createdAt") and datetime.utcfromtimestamp(job["createdAt"]/1000).isoformat() or "",
            "tags": json.dumps([], ensure_ascii=False),
        })
    return out


# ---- Workday ----
# Workday é›†ç¾¤æ£€æµ‹ï¼šwd1-wd15 é€ä¸ªå°è¯•ï¼›æ‰¾åˆ°èƒ½ 200 çš„ cxs æ¥å£å³ç”¨ä¹‹
_WORKDAY_CLUSTERS = [str(i) for i in range(1, 16)]

def _workday_cxs_url(cluster: str, tenant: str, site: str) -> str:
    return f"https://{tenant}.wd{cluster}.myworkdayjobs.com/wday/cxs/{tenant}/{site}/jobs"

def detect_workday_cluster(tenant: str, site: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0 (LGWORK/1.0)"}
    payload = {"appliedFacets": {}, "limit": 1, "offset": 0, "searchText": ""}
    for c in _WORKDAY_CLUSTERS:
        url = _workday_cxs_url(c, tenant, site)
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=12)
            if r.status_code == 200 and "jobPostings" in r.text:
                _log(f"Workday cluster detected: wd{c}")
                return c
        except Exception:
            pass
    raise RuntimeError("æ— æ³•è‡ªåŠ¨è¯†åˆ« Workday é›†ç¾¤ï¼ˆwdNï¼‰ã€‚è¯·æ£€æŸ¥ tenant/site æ˜¯å¦æ­£ç¡®ã€‚")


def parse_workday_line(tenant: str, site: str, cluster: str, j: Dict[str, Any]) -> Dict[str, str]:
    """
    ç»Ÿä¸€çš„ Workday job è§£æå‡½æ•°ï¼ˆä¿®å¤ä½ æ—¥å¿—é‡Œ _parse_workday_line æœªå®šä¹‰çš„é—®é¢˜ï¼‰ã€‚
    cxs è¿”å›å­—æ®µå¸¸è§ï¼štitle, locationsText, externalPath, postedOn
    """
    title = (j.get("title") or "").strip()
    location = (j.get("locationsText") or "").strip()
    path = j.get("externalPath") or j.get("externalUrl") or ""
    path = str(path).lstrip("/")

    # å¸¸è§ externalPath å·²è‡ªå¸¦ job/xxx/xxxï¼Œä¸å†é‡å¤æ‹¼æ¥
    if path.startswith("job/"):
        jd_url = f"https://{tenant}.wd{cluster}.myworkdayjobs.com/{site}/{path}"
    else:
        jd_url = f"https://{tenant}.wd{cluster}.myworkdayjobs.com/{site}/job/{path}"

    posted = (j.get("postedOn") or "").strip()
    return {
        "title": title,
        "company": tenant,
        "location": location,
        "jd_url": jd_url,
        "source": "workday",
        "posted_at": posted,
        "tags": json.dumps([], ensure_ascii=False),
    }


def fetch_workday(tenant: str, site: str, limit: int = 100) -> List[Dict[str, str]]:
    cluster = detect_workday_cluster(tenant, site)
    headers = {"User-Agent": "Mozilla/5.0 (LGWORK/1.0)"}
    url = _workday_cxs_url(cluster, tenant, site)

    out = []
    offset = 0
    step = 50
    while offset < limit:
        payload = {"appliedFacets": {}, "limit": step, "offset": offset, "searchText": ""}
        _log(f"Fetching Workday: {url} offset={offset}")
        r = requests.post(url, headers=headers, json=payload, timeout=20)
        r.raise_for_status()
        data = r.json() or {}
        posts = data.get("jobPostings", [])
        if not posts:
            break
        for j in posts:
            out.append(parse_workday_line(tenant, site, cluster, j))
        if len(posts) < step:
            break
        offset += step

    return out


# ---------- LLMï¼ˆå¯é€‰ï¼‰ ----------
def llm_analyze(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    å…œåº•ï¼šæ²¡é… DeepSeek ä¹Ÿèƒ½è¿”å›ä¸€ä¸ªå¯ç”¨çš„å ä½åˆ†æï¼Œé¿å… 500ã€‚
    å‰ç«¯åªè¦èƒ½æ‹¿åˆ°ç»“æ„åŒ– JSON å°±è¡Œã€‚
    """
    resume_text = (payload.get("resume_text") or "").strip()
    jd_text = (payload.get("jd_text") or "").strip()

    if not LLM_API_KEY:
        # å ä½è¿”å›
        return {
            "model": "placeholder",
            "score": 68,
            "summary": "æœªé…ç½® DeepSeek APIï¼Œè¿”å›å ä½åˆ†æç»“æœã€‚",
            "keywords": ["experience", "skills", "match"],
            "risks": ["æ•™è‚²èƒŒæ™¯å¯èƒ½ä¸å®Œå…¨åŒ¹é…", "ç®¡ç†è·¨åº¦éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤"],
            "advice": ["è¡¥å……å¯é‡åŒ–æˆæœ", "å¯¹é½ JD çš„ must-have èƒ½åŠ›ï¼Œå®Œå–„æ¡ˆä¾‹"],
        }

    # çœŸæ­£è°ƒç”¨ DeepSeekï¼ˆå…¼å®¹ OpenAI é£æ ¼ï¼‰
    try:
        url = f"{LLM_API_BASE}/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {LLM_API_KEY}",
            "Content-Type": "application/json",
        }
        sys_prompt = (
            "You are a recruiting co-pilot. Read resume and JD, return a concise JSON with fields: "
            "score(0-100), summary, keywords(list), risks(list), advice(list). Keep it short."
        )
        user_prompt = f"RESUME:\n{resume_text}\n\nJD:\n{jd_text}\n"
        body = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.2,
        }
        r = requests.post(url, headers=headers, json=body, timeout=60)
        r.raise_for_status()
        resp = r.json()
        content = resp["choices"][0]["message"]["content"]
        # å°è¯•ä»å†…å®¹ä¸­æå– JSONï¼›è‹¥ä¸æ˜¯ä¸¥æ ¼ JSONï¼Œåˆ™åŒ…ä¸€å±‚
        try:
            parsed = json.loads(content)
        except Exception:
            parsed = {
                "model": LLM_MODEL,
                "raw": content,
                "score": 70,
                "summary": "LLM è¿”å›éä¸¥æ ¼ JSONï¼Œå·²å…œåº•ã€‚",
                "keywords": ["AI", "NLP"],
                "risks": [],
                "advice": [],
            }
        return parsed
    except Exception as e:
        _log(f"DeepSeek call failed: {e}")
        return {
            "model": "error-fallback",
            "score": 65,
            "summary": "DeepSeek è°ƒç”¨å¤±è´¥ï¼Œè¿”å›å…œåº•ç»“æœã€‚",
            "keywords": ["fallback"],
            "risks": ["å¤–éƒ¨æ¨¡å‹ä¸å¯ç”¨"],
            "advice": ["ç¨åé‡è¯•æˆ–æ£€æŸ¥ API Key / Base / Model"],
        }


# ---------- è·¯ç”± ----------
@app.route("/")
def index():
    # æ ¹è·¯å¾„ç›´æ¥æ¸²æŸ“ agent.htmlï¼ˆå·²è§£å†³ä½ ä¹‹å‰çš„â€œ/ è¿”å› 404â€ï¼‰
    return send_from_directory(PUBLIC_DIR, "agent.html")


@app.route("/admin")
def admin_page():
    return send_from_directory(PUBLIC_DIR, "admin.html")


@app.route("/api/ping")
def ping():
    return jsonify({"ok": True, "ts": datetime.utcnow().isoformat()})


@app.route("/api/jobs", methods=["GET"])
def list_jobs():
    jobs = _read_jobs()
    return jsonify({"ok": True, "count": len(jobs), "jobs": jobs})


@app.route("/api/clear_jobs", methods=["POST"])
def clear_jobs():
    if not _require_admin(request):
        return jsonify({"ok": False, "error": "Unauthorized"}), 401
    _write_jobs([])
    return jsonify({"ok": True, "cleared": True})


@app.route("/api/ingest_ats", methods=["POST"])
def ingest_ats():
    """
    ç»Ÿä¸€å¯¼å…¥å…¥å£ï¼ˆadmin.html ä¼š POST åˆ°è¿™é‡Œï¼‰
    æ¥æ”¶ JSONï¼š
    {
      "provider": "greenhouse" | "lever" | "workday",
      // Greenhouse/Lever:
      "company": "xxx",
      // Workday:
      "tenant_site": "beigene/BeiGene"   // æŒ‰ä½ è¦æ±‚çš„è¾“å…¥æ ¼å¼
      // å…¼å®¹å‚æ•°ï¼šä¹Ÿå¯ä¼  {"tenant": "...", "site": "..."}
    }
    """
    try:
        if not _require_admin(request):
            return jsonify({"ok": False, "error": "Unauthorized"}), 401

        body = request.get_json(force=True) or {}
        provider = (body.get("provider") or "").strip().lower()
        _log(f"/api/ingest_ats provider={provider}")

        new_jobs: List[Dict[str, str]] = []

        if provider == "greenhouse":
            company = (body.get("company") or "").strip()
            if not company:
                return jsonify({"ok": False, "error": "ç¼ºå°‘ company"}), 400
            new_jobs = fetch_greenhouse(company)

        elif provider == "lever":
            company = (body.get("company") or "").strip()
            if not company:
                return jsonify({"ok": False, "error": "ç¼ºå°‘ company"}), 400
            new_jobs = fetch_lever(company)

        elif provider == "workday":
            # æ”¯æŒ tenant_site æˆ–è€… tenant+site
            tenant_site = (body.get("tenant_site") or body.get("workday") or "").strip()
            tenant = (body.get("tenant") or "").strip()
            site = (body.get("site") or "").strip()
            if tenant_site and (not tenant or not site):
                if "/" in tenant_site:
                    tenant, site = tenant_site.split("/", 1)
            if not (tenant and site):
                return jsonify({"ok": False, "error": "ç¼ºå°‘ tenant/siteï¼Œè¯·æŒ‰ 'tenant/site' æˆ–åˆ†åˆ«æä¾›"}), 400
            new_jobs = fetch_workday(tenant, site, limit=int(body.get("limit") or 200))

        else:
            return jsonify({"ok": False, "error": "æœªçŸ¥ provider"}), 400

        added, skipped = _append_jobs(new_jobs)
        return jsonify({"ok": True, "fetched": len(new_jobs), "added": added, "skipped": skipped})

    except requests.HTTPError as he:
        return jsonify({"ok": False, "error": f"HTTP {he.response.status_code}: {str(he)}"}), 502
    except Exception as e:
        _log(f"ERROR in /api/ingest_ats: {e}")
        return jsonify({"ok": False, "error": f"{type(e).__name__}: {e}"}), 500


@app.route("/api/analyze_recommend", methods=["POST"])
def analyze_recommend():
    """
    å‰ç«¯ä¼šæäº¤ { resume_text, jd_text, ... }
    æˆ‘ä»¬åšå®¹é”™ï¼šå³ä½¿ jobs.csv çš„ tags æ˜¯ç©º/è„ JSONï¼Œä¹Ÿä¸æŠ¥é”™ã€‚
    """
    try:
        body = request.get_json(force=True) or {}
        result = llm_analyze(body)
        return jsonify({"ok": True, "result": result})
    except Exception as e:
        _log(f"ERROR in /api/analyze_recommend: {e}")
        # å…œåº•ï¼Œé¿å… 500
        return jsonify({
            "ok": True,
            "result": {
                "model": "server-fallback",
                "score": 66,
                "summary": "åˆ†æå‡ºç°å¼‚å¸¸ï¼Œå·²è¿”å›å…œåº•ç»“æœã€‚",
                "keywords": [],
                "risks": [],
                "advice": [],
            }
        })


# ---------- é™æ€èµ„æºï¼ˆå…œåº•ï¼‰ ----------
@app.route("/public/<path:filename>")
def public_files(filename):
    # è®© /public/* æ­£å¸¸è¿”å›æ–‡ä»¶ï¼ˆadmin.html / agent.html / JS / CSSï¼‰
    return send_from_directory(PUBLIC_DIR, filename)


# ---------- å¯åŠ¨ ----------
if __name__ == "__main__":
    port = int(os.getenv("PORT", "10000"))
    _log("////////////////////////////////////////////////////")
    _log(f"==> {APP_NAME} is booting on port {port}")
    _log("==> Your service is live  ğŸš€")
    _log("////////////////////////////////////////////////////")
    app.run(host="0.0.0.0", port=port)
